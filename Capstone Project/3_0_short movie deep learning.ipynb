{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has  5332  postive movies reviews\n",
      "has  5332  negative movies reviews\n",
      "all documents is  10664\n",
      "['the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth . ', 'effective but too-tepid biopic']\n"
     ]
    }
   ],
   "source": [
    "# loading and cleaning reviews\n",
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# read the training txt    \n",
    "short_pos = open (\"C:\\software\\WinPython-64bit-3.6.2.0Qt5\\\\notebooks\\capstone\\shortmoviereview\\positive.txt\").read()\n",
    "short_neg = open (\"C:\\software\\WinPython-64bit-3.6.2.0Qt5\\\\notebooks\\capstone\\shortmoviereview\\\\negative.txt\").read()\n",
    "\n",
    "#split into paragraph and append a tag\n",
    "documents_pos = []\n",
    "documents_neg = []\n",
    "\n",
    "for r in short_pos.split('\\n'):\n",
    "    documents_pos.append(r)\n",
    "    \n",
    "for r in short_neg.split('\\n'):\n",
    "    documents_neg.append(r)\n",
    "    \n",
    "documents = documents_pos + documents_neg\n",
    "print(\"has \",len(documents_pos),\" postive movies reviews\")\n",
    "print(\"has \",len(documents_neg),\" negative movies reviews\")\n",
    "print(\"all documents is \",len(documents))    \n",
    "print(documents[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequence of the words\n",
    "# We can step through the vocabulary and remove all words that have a low occurrence, such as only being used once or twice in all reviews.\n",
    "\n",
    "all_words = []\n",
    "\n",
    "short_pos_words = word_tokenize(short_pos)\n",
    "short_neg_words = word_tokenize(short_neg)\n",
    "\n",
    "for w in short_pos_words:\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "for w in short_neg_words:\n",
    "    all_words.append(w.lower())\n",
    "\n",
    "#remove the stop words and punctuation.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = [w for w in all_words if not w in stop_words and w not in string.punctuation]\n",
    "\n",
    "all_words = nltk.FreqDist(filtered_words)\n",
    "\n",
    "all_words = [k for k,c in all_words.items() if c >=2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save word_features\n",
    "save_word_features = open(\"all_words.pickle\",\"wb\")\n",
    "pickle.dump(all_words, save_word_features)\n",
    "save_word_features.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens and join into the sentence\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    token_words = list()\n",
    "    for w in tokens:\n",
    "        token_words.append(w.lower())\n",
    "    #remove the stop words and punctuation.\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    clean_words = [w for w in token_words if w in all_words]\n",
    "    clean_words = ' '.join(clean_words)\n",
    "\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"gorgeously elaborate continuation lord rings trilogy huge column words cannot adequately describe co-writer/director peter jackson's expanded vision j r r tolkien's middle-earth\", 'effective too-tepid biopic']\n"
     ]
    }
   ],
   "source": [
    "# get the filtered sentences which have removed the stopwords and punctuation\n",
    "filter_sentence= list()\n",
    "for d in documents:\n",
    "    sentence = clean_doc(d)\n",
    "    filter_sentence.append(sentence)\n",
    "\n",
    "    \n",
    "print(filter_sentence[1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the filter_sentence\n",
    "save_sentence = open(\"filter_sentence.pickle\",\"wb\")\n",
    "pickle.dump(filter_sentence, save_sentence)\n",
    "save_sentence.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3228, 2053, 7158, 4517, 2823, 4518, 957, 7159, 706, 769, 4519, 2513, 573, 162, 16, 958, 5502, 4520, 574, 1178, 1887, 1887, 7160, 372, 1370]\n"
     ]
    }
   ],
   "source": [
    "# training documents as sequences of integers using the Tokenizer class in the Keras API\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(filter_sentence)\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(filter_sentence)\n",
    "print(encoded_docs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3228 2053 7158 4517 2823 4518  957 7159  706  769 4519 2513  573  162\n",
      "    16  958 5502 4520  574 1178 1887 1887 7160  372 1370    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [ 607 1371 2514 2515    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# pad sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_length = max([len(s.split()) for s in filter_sentence])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(Xtrain[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the trainning and testing datasets\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "x_pos_train=Xtrain[0:4000]\n",
    "x_pos_test=Xtrain[4000:5332]\n",
    "x_neg_train=Xtrain[5332:9332]\n",
    "x_neg_test=Xtrain[9332:]\n",
    "\n",
    "x_train=np.append(x_pos_train, x_neg_train,axis=0)\n",
    "y_train=array([1 for _ in range(4000)] + [0 for _ in range(4000)])\n",
    "\n",
    "x_test=np.append(x_pos_test, x_neg_test,axis=0)\n",
    "y_test=array([1 for _ in range(1332)] + [0 for _ in range(1332)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19432\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation\n",
    "\n",
    "### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 39, 50)            971600    \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 39, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 37, 64)            9664      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 18, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 18, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 100)               115300    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,096,665\n",
      "Trainable params: 1,096,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dropout\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(7)\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(vocab_size, 50, input_length=max_length))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model1.add(MaxPooling1D(pool_size=2))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(100, activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6s - loss: 0.6932 - acc: 0.5044\n",
      "Epoch 2/3\n",
      "3s - loss: 0.5781 - acc: 0.7069\n",
      "Epoch 3/3\n",
      "3s - loss: 0.3141 - acc: 0.8656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2366d165780>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compile network\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model1.fit(x_train, y_train, epochs=3, verbose=2, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 75.638138\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "loss, acc = model1.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 39, 50)            971600    \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 991,851\n",
      "Trainable params: 991,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 8s - loss: 0.6937 - acc: 0.4835     \n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 5s - loss: 0.6931 - acc: 0.5032     \n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.6933 - acc: 0.4959     \n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.6665 - acc: 0.5769     \n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.5043 - acc: 0.7949     \n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.3892 - acc: 0.8608     \n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.3353 - acc: 0.8838     \n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.2754 - acc: 0.9115     \n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 4s - loss: 0.2486 - acc: 0.9287     \n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 5s - loss: 0.2227 - acc: 0.9374     \n",
      "Accuracy: 74.06%\n"
     ]
    }
   ],
   "source": [
    "## using LSTM\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, 50, input_length=max_length))\n",
    "model2.add(LSTM(50, dropout=0.4, recurrent_dropout=0.4))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model2.summary())\n",
    "model2.fit(x_train, y_train, epochs=10, batch_size=256,shuffle=True)\n",
    "# Final evaluation of the model\n",
    "scores = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 39, 50)            971600    \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 39, 32)            4832      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 19, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,029,733\n",
      "Trainable params: 1,029,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "8000/8000 [==============================] - 7s - loss: 0.6698 - acc: 0.5534     \n",
      "Epoch 2/3\n",
      "8000/8000 [==============================] - 5s - loss: 0.3361 - acc: 0.8600     \n",
      "Epoch 3/3\n",
      "8000/8000 [==============================] - 5s - loss: 0.0957 - acc: 0.9690     \n",
      "Accuracy: 73.76%\n"
     ]
    }
   ],
   "source": [
    "# # LSTM and CNN\n",
    "\n",
    "# model3 = Sequential()\n",
    "# model3.add(Embedding(vocab_size, 50, input_length=max_length))\n",
    "# model3.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model3.add(MaxPooling1D(pool_size=2))\n",
    "# model3.add(LSTM(100))\n",
    "# model3.add(Dense(1, activation='sigmoid'))\n",
    "# model3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model3.summary())\n",
    "# model3.fit(x_train, y_train, epochs=3, batch_size=128)\n",
    "# # Final evaluation of the model\n",
    "# scores = model3.evaluate(x_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights\n",
    "cnn = \"cnn.hdf5\"\n",
    "model1.save_weights(cnn,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = \"lstm.hdf5\"\n",
    "model2.save_weights(lstm,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
